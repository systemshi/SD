{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1hWWwKAHAC8a4THjKmEM0_j6RnyX37dYl","authorship_tag":"ABX9TyNHZrtRIxcQK7iyVftGqorR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"50db8afc2f9c4af589934f34101f3701":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0133a9a5daf84fb3a9b0c2b46c693d70","IPY_MODEL_e6f202bb8a42481397925713c7b58abb","IPY_MODEL_f222b49a66114b05843a4b3a51952b33"],"layout":"IPY_MODEL_ecd7086bd34b430080f2b7aa64a9bc02"}},"0133a9a5daf84fb3a9b0c2b46c693d70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db99e9f8caba40ee85b05e066808c4db","placeholder":"​","style":"IPY_MODEL_6d71cbb089dd4cbb8e3e3f870c55fe46","value":"Loading checkpoint shards: 100%"}},"e6f202bb8a42481397925713c7b58abb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_763af02df1594ee4a19a8aee69f46a36","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8f5dafc5c66444c9e777633b6136fcb","value":4}},"f222b49a66114b05843a4b3a51952b33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e478eff2f08d4d1abbd3072ee689e78b","placeholder":"​","style":"IPY_MODEL_9c25fbf7673e48768c590adb5f23e222","value":" 4/4 [00:13&lt;00:00,  3.38s/it]"}},"ecd7086bd34b430080f2b7aa64a9bc02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db99e9f8caba40ee85b05e066808c4db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d71cbb089dd4cbb8e3e3f870c55fe46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"763af02df1594ee4a19a8aee69f46a36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8f5dafc5c66444c9e777633b6136fcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e478eff2f08d4d1abbd3072ee689e78b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c25fbf7673e48768c590adb5f23e222":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# 加载目标模型和草稿模型\n","target_model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n","draft_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(target_model_name, trust_remote_code=True)\n","target_model = AutoModelForCausalLM.from_pretrained(target_model_name, device_map=\"auto\", trust_remote_code=True)\n","draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, device_map=\"auto\", trust_remote_code=True)\n","\n","# 设置模型为评估模式\n","target_model.eval()\n","draft_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":630,"referenced_widgets":["50db8afc2f9c4af589934f34101f3701","0133a9a5daf84fb3a9b0c2b46c693d70","e6f202bb8a42481397925713c7b58abb","f222b49a66114b05843a4b3a51952b33","ecd7086bd34b430080f2b7aa64a9bc02","db99e9f8caba40ee85b05e066808c4db","6d71cbb089dd4cbb8e3e3f870c55fe46","763af02df1594ee4a19a8aee69f46a36","c8f5dafc5c66444c9e777633b6136fcb","e478eff2f08d4d1abbd3072ee689e78b","9c25fbf7673e48768c590adb5f23e222"]},"id":"xMFr3WNBmIBn","executionInfo":{"status":"ok","timestamp":1734866515377,"user_tz":-660,"elapsed":24957,"user":{"displayName":"Yansong Shi","userId":"18060374193842448956"}},"outputId":"4c4b1a46-bfaf-46dc-aa41-6d4774b090e7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50db8afc2f9c4af589934f34101f3701"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["Qwen2ForCausalLM(\n","  (model): Qwen2Model(\n","    (embed_tokens): Embedding(151936, 896)\n","    (layers): ModuleList(\n","      (0-23): 24 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2SdpaAttention(\n","          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n","          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n","          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n","          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n","          (rotary_emb): Qwen2RotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n","          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n","          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n","        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n","    (rotary_emb): Qwen2RotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# 定义speculative decoding函数\n","def speculative_decoding(prompt, max_length=50, alpha=2):\n","    # 使用草稿模型生成初始序列\n","    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(target_model.device)\n","    #input_ids: [1, 6]\n","    with torch.no_grad():\n","        draft_outputs = draft_model.generate(\n","            input_ids,\n","            max_length=max_length,\n","            do_sample=True,\n","            top_k=50,\n","            temperature=0.7,\n","            eos_token_id=tokenizer.eos_token_id,\n","            output_scores=True,\n","            return_dict_in_generate=True\n","        )\n","    #draft_outputs.sequences: [1, 50], 109924, 104133,...\n","\n","    # 获取草稿模型生成的token序列\n","    draft_tokens = draft_outputs.sequences[0]\n","    #draft_tokens: [50], 109924, 104133,...\n","    draft_token_scores = draft_outputs.scores\n","\n","    # 初始化最终输出序列\n","    final_output_ids = input_ids[0]\n","    #final_output_ids: [6]\n","\n","    # 逐步验证草稿模型生成的token\n","    i = 0\n","    #用i表示当前draft相对位置\n","    while i < len(draft_tokens) - len(input_ids[0]):\n","        #50 - 6\n","        print('当前预测绝对位置索引:', len(input_ids[0]) + i)\n","        print('当前预测draft相对位置索引i:', i)\n","        print('当前prompt长度length of input_ids[0]:', len(input_ids[0]))\n","        if len(input_ids[0]) + i >= len(draft_tokens):\n","            print(f\"Index {len(input_ids[0]) + i} is out of bounds for draft_tokens with size {len(draft_tokens)}\")\n","            break\n","        token_id = draft_tokens[len(input_ids[0]) + i].unsqueeze(0)\n","        #print('token_id', token_id)\n","        #token_id: [1]，len(input_ids[0]) + i表示当前绝对位置\n","\n","        # 计算目标模型的token概率\n","        with torch.no_grad():\n","            target_outputs = target_model(final_output_ids.unsqueeze(0))\n","            target_logits = target_outputs.logits[:, -1, :]\n","            #target_logits:[B,V]\n","            #target_outputs.logits:[B,S,V]\n","            target_probs = torch.softmax(target_logits, dim=-1)\n","            #target_probs:[B,V]\n","\n","        # 获取草稿模型的token概率\n","        draft_prob = torch.softmax(draft_token_scores[i], dim=-1)\n","        #draft_prob:[B,S]\n","\n","        # 计算加速比alpha，决定是否接受草稿模型的token\n","        #print('target_probs[0, token_id]:', target_probs[0, token_id])\n","        #print('draft_prob[0, token_id]:', draft_prob[0, token_id])\n","        acceptance_ratio = target_probs[0, token_id] / (alpha * draft_prob[0, token_id])\n","\n","        if acceptance_ratio >= 1:\n","            # 接受草稿模型的token\n","            print('!!!接受!!!')\n","            final_output_ids = torch.cat([final_output_ids, token_id])\n","            #final_output_ids: [x]\n","            i += 1\n","\n","        else:\n","            print('!!!拒绝!!!')\n","            # 使用目标模型采样下一个token\n","            with torch.no_grad():\n","                target_next_token = torch.multinomial(target_probs, num_samples=1)\n","            final_output_ids = torch.cat([final_output_ids, target_next_token.squeeze(0)])\n","\n","            # 更新草稿模型的输入\n","            input_ids = final_output_ids.unsqueeze(0)\n","            #input_ids: [1, x]\n","            i = 0\n","\n","\n","    # 解码最终的输出序列\n","    generated_text = tokenizer.decode(final_output_ids, skip_special_tokens=True)\n","    return generated_text"],"metadata":{"id":"QDtg1QqT6rit","executionInfo":{"status":"ok","timestamp":1734866515377,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yansong Shi","userId":"18060374193842448956"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    prompt = \"从前有一个年轻的程序员，他\"\n","    output_text = speculative_decoding(prompt)\n","    print(\"生成的文本：\")\n","    print(output_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtb21Baa0Q2L","outputId":"f9845f5a-96dc-4f86-dd05-dd41de87e7e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"]},{"output_type":"stream","name":"stdout","text":["当前预测绝对位置索引: 6\n","当前预测draft相对位置索引i: 0\n","当前prompt长度length of input_ids[0]: 6\n","!!!拒绝!!!\n","当前预测绝对位置索引: 7\n","当前预测draft相对位置索引i: 0\n","当前prompt长度length of input_ids[0]: 7\n","!!!接受!!!\n","当前预测绝对位置索引: 8\n","当前预测draft相对位置索引i: 1\n","当前prompt长度length of input_ids[0]: 7\n","!!!接受!!!\n","当前预测绝对位置索引: 9\n","当前预测draft相对位置索引i: 2\n","当前prompt长度length of input_ids[0]: 7\n","!!!接受!!!\n"]}]}]}